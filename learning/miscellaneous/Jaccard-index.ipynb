{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb3fc9f",
   "metadata": {},
   "source": "# 📊 Índice de Jaccard: Medición de Similitud entre Conjuntos\n\nEste notebook explora el **Índice de Jaccard**, una métrica fundamental en análisis de datos para cuantificar la similitud entre conjuntos."
  },
  {
   "cell_type": "markdown",
   "id": "9bac0372",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": "## 🎯 ¿Qué es el Índice de Jaccard?\n\nEl **Índice de Jaccard** ($I_J$), también conocido como **Coeficiente de Jaccard**, es una métrica estadística utilizada para medir el grado de similitud entre dos conjuntos finitos, independientemente del tipo de elementos que contengan.\n\n### 📐 Formulación Matemática\n\n$$\nJ(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}\n$$\n\nDonde:\n- $|A \\cap B|$ representa la cardinalidad de la **intersección** de ambos conjuntos (elementos comunes)\n- $|A \\cup B|$ representa la cardinalidad de la **unión** de ambos conjuntos (elementos totales únicos)\n\n### 📈 Propiedades\n\n- **Rango**: El índice siempre toma valores entre **0** y **1**\n  - $J = 0$: Los conjuntos son completamente disjuntos (sin elementos en común)\n  - $J = 1$: Los conjuntos son idénticos (igualdad total)\n- **Simetría**: $J(A, B) = J(B, A)$\n- **Interpretación**: A mayor valor, mayor similitud entre los conjuntos"
  },
  {
   "cell_type": "markdown",
   "id": "34cdd028",
   "metadata": {},
   "source": "---\n\n## 💡 Caso de Uso 1: Detección de Clusters Duplicados\n\nEn este ejemplo práctico, utilizamos el índice de Jaccard para identificar clusters de mensajes que son prácticamente duplicados, basándonos en sus palabras clave (*keywords*). Esta técnica es especialmente útil en:\n\n- **Análisis de redes sociales**: Agrupar conversaciones similares\n- **Sistemas de recomendación**: Evitar contenido redundante\n- **Procesamiento de texto**: Deduplicación de documentos"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faf3b038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters originales:\n",
      "0: {'model', 'data', 'training', 'ai'}\n",
      "1: {'learning', 'network', 'neural', 'deep'}\n",
      "2: {'model', 'dataset', 'training', 'ai'}\n",
      "3: {'meeting', 'break', 'coffee'}\n",
      "4: {'learning', 'network', 'neural', 'deep'}\n",
      "\n",
      "Posibles duplicados (índice1, índice2, similitud):\n",
      "(1, 4, 1.0)\n",
      "\n",
      "Clusters después de eliminar duplicados:\n",
      "{'model', 'data', 'training', 'ai'}\n",
      "{'learning', 'network', 'neural', 'deep'}\n",
      "{'model', 'dataset', 'training', 'ai'}\n",
      "{'meeting', 'break', 'coffee'}\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo: Uso del índice de Jaccard para detectar clusters duplicados\n",
    "# ===============================================================\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"\n",
    "    Calcula la similitud de Jaccard entre dos conjuntos.\n",
    "    \"\"\"\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "\n",
    "# Supongamos que tenemos clusters de mensajes representados por keywords\n",
    "clusters = [\n",
    "    {\"ai\", \"model\", \"training\", \"data\"},\n",
    "    {\"deep\", \"learning\", \"neural\", \"network\"},\n",
    "    {\"ai\", \"model\", \"training\", \"dataset\"},\n",
    "    {\"coffee\", \"break\", \"meeting\"},\n",
    "    {\"neural\", \"network\", \"deep\", \"learning\"},\n",
    "]\n",
    "\n",
    "# Umbral de similitud para considerar duplicados\n",
    "threshold = 0.7\n",
    "\n",
    "# Detectar duplicados\n",
    "duplicates = []\n",
    "visited = set()\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    for j in range(i+1, len(clusters)):\n",
    "        sim = jaccard_similarity(clusters[i], clusters[j])\n",
    "        if sim >= threshold:\n",
    "            duplicates.append((i, j, sim))\n",
    "            visited.add(j)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Clusters originales:\")\n",
    "for idx, c in enumerate(clusters):\n",
    "    print(f\"{idx}: {c}\")\n",
    "\n",
    "print(\"\\nPosibles duplicados (índice1, índice2, similitud):\")\n",
    "for dup in duplicates:\n",
    "    print(dup)\n",
    "\n",
    "# Filtrar clusters únicos\n",
    "unique_clusters = [c for idx, c in enumerate(clusters) if idx not in visited]\n",
    "\n",
    "print(\"\\nClusters después de eliminar duplicados:\")\n",
    "for c in unique_clusters:\n",
    "    print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba81197",
   "metadata": {},
   "source": "---\n\n## 🛒 Caso de Uso 2: Sistemas de Recomendación de Productos\n\nEn este ejemplo, aplicamos el índice de Jaccard para construir un **motor de recomendación colaborativo** basado en el historial de compras de usuarios. \n\n### 🎯 Objetivo\n\nMedir la similitud entre usuarios según sus productos adquiridos para:\n- Recomendar productos que compraron usuarios similares\n- Identificar patrones de compra\n- Personalizar la experiencia de compra\n\n### 🔍 Metodología\n\nComparamos los conjuntos de productos de diferentes usuarios y utilizamos el índice de Jaccard como métrica de similitud para determinar qué usuarios tienen preferencias similares."
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d24f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre Usuario A y Usuario B: 0.4\n",
      "Similitud entre Usuario A y Usuario C: 0.0\n",
      "\n",
      "Usuario A y B son más similares → recomendar productos de B a A\n"
     ]
    }
   ],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"\n",
    "    Calcula la similitud de Jaccard entre dos conjuntos.\n",
    "    \"\"\"\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "\n",
    "# Historial de compras de usuarios\n",
    "user_A = {\"laptop\", \"mouse\", \"keyboard\", \"monitor\"}\n",
    "user_B = {\"laptop\", \"mouse\", \"tablet\"}\n",
    "user_C = {\"shoes\", \"t-shirt\", \"jeans\"}\n",
    "\n",
    "# Calcular similitud\n",
    "sim_A_B = jaccard_similarity(user_A, user_B)\n",
    "sim_A_C = jaccard_similarity(user_A, user_C)\n",
    "\n",
    "print(\"Similitud entre Usuario A y Usuario B:\", sim_A_B)\n",
    "print(\"Similitud entre Usuario A y Usuario C:\", sim_A_C)\n",
    "\n",
    "# Decidir recomendaciones\n",
    "if sim_A_B > sim_A_C:\n",
    "    print(\"\\nUsuario A y B son más similares → recomendar productos de B a A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2891e",
   "metadata": {},
   "outputs": [],
   "source": "# 🔍 Caso de Uso 3: Detección de Preguntas Similares mediante Tokenización\n# ============================================================================\n# Este análisis identifica preguntas redundantes en un conjunto de preguntas\n# para eventos, utilizando tokenización simple (división por palabras).\n\ndef jaccard_similarity(set1, set2):\n    \"\"\"\n    Calcula la similitud de Jaccard entre dos conjuntos de palabras.\n    \"\"\"\n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n    return intersection / union if union != 0 else 0\n\n\n# Preguntas para un evento sobre ciencia de datos y experiencias personales\nquestions = [\n    \"¿Cuál ha sido tu mayor reto aprendiendo ciencia de datos?\",\n    \"¿Cuál ha sido tu mayor reto personal aprendiendo ciencia de datos avanzado?\",\n    \"¿Puedes compartir una experiencia personal aplicando ciencia de datos en tu trabajo?\",\n    \"¿Cuál consideras la habilidad más importante para un científico de datos?\",\n    \"¿Qué consejo le darías a alguien que empieza en ciencia de datos?\",\n    \"¿Cómo enfrentaste dificultades en proyectos de ciencia de datos pasados?\",\n    \"¿Qué frameworks prefieres para machine learning y por qué?\",\n    \"¿Cuál ha sido tu experiencia más significativa aplicando modelos predictivos?\",\n    \"¿Qué aprendiste de tus primeros proyectos en ciencia de datos?\",\n    \"¿Cómo manejas los errores y aprendizajes en proyectos de ciencia de datos?\"\n]\n\n# Convertimos cada pregunta en un conjunto de palabras (tokens)\n# Eliminamos signos de interrogación para el análisis\nquestion_sets = [set(q.lower().replace(\"¿\",\"\").replace(\"?\",\"\").split()) for q in questions]\n\n# Umbral de similitud: solo consideramos pares con similitud >= 0.8\nthreshold = 0.8\n\n# Comparar todas las preguntas entre sí\nsimilar_pairs = []\nfor i in range(len(question_sets)):\n    for j in range(i+1, len(question_sets)):\n        sim = jaccard_similarity(question_sets[i], question_sets[j])\n        if sim >= threshold:\n            similar_pairs.append(((questions[i], questions[j]), sim))\n\n# Mostrar resultados ordenados por similitud (descendente)\nprint(\"🔎 Pares de preguntas similares (threshold =\", threshold, \"):\\n\")\nfor pair, sim in sorted(similar_pairs, key=lambda x: x[1], reverse=True):\n    print(f\"- [{sim:.2f}] '{pair[0]}'\\n          vs\\n          '{pair[1]}'\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1375400",
   "metadata": {},
   "outputs": [],
   "source": "# 🧠 Mejora mediante Lematización con SpaCy\n# ==========================================\n# En este enfoque avanzado, utilizamos lematización para normalizar las palabras\n# (ej: \"aprendiendo\" → \"aprender\", \"datos\" → \"dato\"), eliminando variaciones\n# morfológicas y mejorando la detección de similitud semántica.\n\nimport spacy\n\n# Cargar modelo de lenguaje en español\nnlp = spacy.load(\"es_core_news_sm\")\n\ndef lemmatize_text(text):\n    \"\"\"\n    Recibe un string y devuelve un conjunto de lemas (lemmas).\n    Elimina stopwords y signos de puntuación para optimizar el análisis.\n    \"\"\"\n    doc = nlp(text.lower().replace(\"¿\",\"\").replace(\"?\",\"\"))\n    return {token.lemma_ for token in doc if not token.is_punct and not token.is_stop}\n\n# Función Jaccard\ndef jaccard_similarity(set1, set2):\n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n    return intersection / union if union != 0 else 0\n\n\n# 🔹 20 preguntas (10 originales + 10 nuevas variantes)\nquestions = [\n    # === Preguntas originales ===\n    \"¿Cuál ha sido tu mayor reto aprendiendo ciencia de datos?\",\n    \"¿Cuál ha sido tu mayor reto personal aprendiendo ciencia de datos avanzado?\",\n    \"¿Puedes compartir una experiencia personal aplicando ciencia de datos en tu trabajo?\",\n    \"¿Cuál consideras la habilidad más importante para un científico de datos?\",\n    \"¿Qué consejo le darías a alguien que empieza en ciencia de datos?\",\n    \"¿Cómo enfrentaste dificultades en proyectos de ciencia de datos pasados?\",\n    \"¿Qué frameworks prefieres para machine learning y por qué?\",\n    \"¿Cuál ha sido tu experiencia más significativa aplicando modelos predictivos?\",\n    \"¿Qué aprendiste de tus primeros proyectos en ciencia de datos?\",\n    \"¿Cómo manejas los errores y aprendizajes en proyectos de ciencia de datos?\",\n\n    # === Nuevas variantes (algunas muy similares, otras distintas) ===\n    \"¿Cuál fue tu mayor reto cuando comenzaste a estudiar ciencia de datos?\",\n    \"¿Puedes contar una experiencia positiva aplicando ciencia de datos en un proyecto?\",\n    \"¿Qué habilidades blandas consideras necesarias para trabajar en ciencia de datos?\",\n    \"¿Qué opinas sobre la importancia de la ética en proyectos de inteligencia artificial?\",\n    \"¿Cuál ha sido la herramienta más útil para ti en proyectos de machine learning?\",\n    \"¿Qué recomendarías a alguien que quiere especializarse en big data?\",\n    \"¿Cómo describirías tu curva de aprendizaje en ciencia de datos?\",\n    \"¿Qué importancia le das al trabajo en equipo en proyectos de ciencia de datos?\",\n    \"¿Cuál es tu framework favorito para deep learning y por qué?\",\n    \"¿Qué retos ves en el futuro de la ciencia de datos en América Latina?\"\n]\n\n\n# Convertimos cada pregunta a conjunto de lemas\nquestion_sets = [lemmatize_text(q) for q in questions]\n\n# Umbral de similitud reducido a 0.5 para captar más variaciones\nthreshold = 0.5\n\n# Comparar todas las preguntas\nsimilar_pairs = []\nfor i in range(len(question_sets)):\n    for j in range(i+1, len(question_sets)):\n        sim = jaccard_similarity(question_sets[i], question_sets[j])\n        if sim >= threshold:\n            similar_pairs.append(((questions[i], questions[j]), sim))\n\n# Mostrar resultados ordenados por similitud\nprint(\"🔎 Pares de preguntas similares con lematización (threshold =\", threshold, \"):\\n\")\nfor pair, sim in sorted(similar_pairs, key=lambda x: x[1], reverse=True):\n    print(f\"- [{sim:.2f}] '{pair[0]}'\\n          vs\\n          '{pair[1]}'\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3519f6a",
   "metadata": {},
   "outputs": [],
   "source": "# 🎯 Clustering Automático de Preguntas Similares\n# ================================================\n# Este algoritmo agrupa preguntas similares en clusters utilizando el índice de Jaccard\n# con lematización. Es útil para organizar preguntas en categorías temáticas automáticamente.\n\nimport spacy\n\n# Cargar modelo en español\nnlp = spacy.load(\"es_core_news_sm\")\n\ndef lemmatize_text(text):\n    \"\"\"\n    Recibe un string y devuelve un conjunto de lemas (lemmas).\n    \"\"\"\n    doc = nlp(text.lower().replace(\"¿\",\"\").replace(\"?\",\"\"))\n    return {token.lemma_ for token in doc if not token.is_punct and not token.is_stop}\n\n# Función Jaccard\ndef jaccard_similarity(set1, set2):\n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n    return intersection / union if union != 0 else 0\n\n# 📋 Dataset ampliado: Preguntas generales + temporales\nquestions = [\n    # === Preguntas sobre experiencia y aprendizaje ===\n    \"¿Cuál ha sido tu mayor reto aprendiendo ciencia de datos?\",\n    \"¿Cuál ha sido tu mayor reto personal aprendiendo ciencia de datos avanzado?\",\n    \"¿Puedes compartir una experiencia personal aplicando ciencia de datos en tu trabajo?\",\n    \"¿Cuál consideras la habilidad más importante para un científico de datos?\",\n    \"¿Qué consejo le darías a alguien que empieza en ciencia de datos?\",\n    \"¿Cómo enfrentaste dificultades en proyectos de ciencia de datos pasados?\",\n    \"¿Qué frameworks prefieres para machine learning y por qué?\",\n    \"¿Cuál ha sido tu experiencia más significativa aplicando modelos predictivos?\",\n    \"¿Qué aprendiste de tus primeros proyectos en ciencia de datos?\",\n    \"¿Cómo manejas los errores y aprendizajes en proyectos de ciencia de datos?\",\n    \"¿Cuál fue tu mayor reto cuando comenzaste a estudiar ciencia de datos?\",\n    \"¿Puedes contar una experiencia positiva aplicando ciencia de datos en un proyecto?\",\n    \"¿Qué habilidades blandas consideras necesarias para trabajar en ciencia de datos?\",\n    \"¿Qué opinas sobre la importancia de la ética en proyectos de inteligencia artificial?\",\n    \"¿Cuál ha sido la herramienta más útil para ti en proyectos de machine learning?\",\n    \"¿Qué recomendarías a alguien que quiere especializarse en big data?\",\n    \"¿Cómo describirías tu curva de aprendizaje en ciencia de datos?\",\n    \"¿Qué importancia le das al trabajo en equipo en proyectos de ciencia de datos?\",\n    \"¿Cuál es tu framework favorito para deep learning y por qué?\",\n    \"¿Qué retos ves en el futuro de la ciencia de datos en América Latina?\",\n    \n    # === Preguntas sobre trayectoria temporal ===\n    \"¿A qué edad empezaste a estudiar ciencia de datos?\",\n    \"¿Cuántos años llevas aprendiendo ciencia de datos?\",\n    \"¿Hace cuánto tiempo comenzaste a interesarte en ciencia de datos?\",\n    \"¿Cuánto tiempo te tomó aprender los fundamentos de ciencia de datos?\",\n    \"¿En qué año iniciaste tu carrera en ciencia de datos?\",\n    \"¿Cuánto tiempo dedicas semanalmente a proyectos de ciencia de datos?\",\n    \"¿Cómo ha evolucionado tu aprendizaje de ciencia de datos a lo largo de los años?\",\n    \"¿Cuántos proyectos personales has hecho desde que comenzaste en ciencia de datos?\",\n    \"¿Cuánto tiempo llevas aplicando ciencia de datos en el trabajo?\",\n    \"¿Hace cuántos años escuchaste por primera vez sobre ciencia de datos?\"\n]\n\n# Convertimos cada pregunta a conjunto de lemas\nquestion_sets = [lemmatize_text(q) for q in questions]\n\n# Umbral de similitud para clustering\nthreshold = 0.65\n\n# 🔹 Algoritmo de clustering simple basado en similitud Jaccard\nclusters = []\nused = set()\n\nfor i in range(len(questions)):\n    if i in used:\n        continue\n    cluster = [(questions[i], question_sets[i])]\n    used.add(i)\n    for j in range(i+1, len(questions)):\n        sim = jaccard_similarity(question_sets[i], question_sets[j])\n        if sim >= threshold:\n            cluster.append((questions[j], question_sets[j]))\n            used.add(j)\n    clusters.append(cluster)\n\n# 📊 Mostrar clusters con formato mejorado\nprint(f\"🎯 Clusters de preguntas similares (threshold = {threshold}):\\n\")\nprint(\"=\" * 80 + \"\\n\")\n\nfor idx, cluster in enumerate(clusters, 1):\n    print(f\"🔹 **Grupo {idx}** ({len(cluster)} pregunta{'s' if len(cluster) > 1 else ''}):\")\n    for q, lemmas in cluster:\n        print(f\"   📌 {q}\")\n        print(f\"      🔤 Lemmas: {lemmas}\")\n    print(\"\\n\" + \"-\" * 80 + \"\\n\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "c1252e56",
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## 🎓 Conclusiones\n\nEl **Índice de Jaccard** es una herramienta poderosa y versátil para medir similitud en diversos contextos:\n\n### ✅ Ventajas\n- **Simplicidad**: Fácil de implementar y comprender\n- **Versatilidad**: Aplicable a cualquier tipo de conjunto (texto, productos, usuarios, etc.)\n- **Interpretabilidad**: Valor normalizado entre 0 y 1, fácil de interpretar\n\n### ⚠️ Limitaciones\n- **Sensibilidad al tamaño**: No considera la frecuencia de elementos\n- **Tokenización básica**: Requiere técnicas adicionales (lematización, embeddings) para mejores resultados en texto\n- **Contexto semántico**: No captura relaciones semánticas profundas (sinónimos, polisemia)\n\n### 🚀 Aplicaciones Prácticas\n1. **Sistemas de recomendación**: Usuarios o productos similares\n2. **Deduplicación**: Detectar contenido redundante\n3. **Análisis de clustering**: Agrupar elementos similares\n4. **Recuperación de información**: Búsqueda de documentos similares\n5. **NLP**: Comparación de textos, análisis de similitud de preguntas\n\n---\n\n📚 **Referencias adicionales**:\n- [Wikipedia: Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index)\n- [SpaCy: Lematización en español](https://spacy.io/models/es)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}